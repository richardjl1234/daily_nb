{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is to ingest the rz1, rz2, rz3, rz4, busi_def, glossary into cloudant database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:**   \n",
    "the mapping /result in docker image is mapped to local folder `~/Documents/GitHub/jupyter/result`  \n",
    "the mapping /input in docker image is mapped to local folder `~/Documents/GitHub/jupyter/input`  \n",
    "\n",
    "\n",
    "link to the cloudant database is here:   \n",
    "https://415ab4c2-a5d1-4acc-bd9a-6ffc2a7ff356-bluemix.cloudant.com/dashboard.html    \n",
    "\n",
    "after the data is ingested, use the following api to refresh the odmmeta api server   \n",
    "https://odmmeta.dal1a.ciocloud.nonprod.intranet.ibm.com/\n",
    "\n",
    "use the following api to test the odmmeta server  \n",
    "https://odmmeta.dal1a.ciocloud.nonprod.intranet.ibm.com/igc/get_term/RCNUM\n",
    "\n",
    "**assumption of this notebook:**  \n",
    "1. the schema for odm is ODMPRD and the schema of ODMR is IDMPRD (the IDMDB is not included yet)\n",
    "1. For those columns which can not be found in sysibm.syscolumns, it will be discarded.\n",
    "1. For those table which can not be found in sysibmn.systables, it will be discarded. \n",
    "1. use the defualt cloudant user id and password. \n",
    "1. we put all RZ1, RZ2, RZ3, RZ4 contents in the same cloudant database.\n",
    "1. the key for RZ1 and RZ3 is the table name (not TID). the key for RZ2 and RZ4 is the table name + column name. (not tid + column name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('/odm_modules')\n",
    "from common_func import cloudant_conn as cc\n",
    "from common_func import odm_conn\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import docx2txt\n",
    "cc.cloudant_client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environ_suffix = '_test'  # it should be either _test or '' \n",
    "environ_suffix = ''  # it should be either _test or '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter_rz2 = { \n",
    "    'CTID': 'table_id',\n",
    "    'ALTER_ID': 'alter_id',\n",
    "    'CTABNAME': 'table_name',\n",
    "    'CCOLNAME': 'column_name',\n",
    "    'CCOLFMT': 'format',\n",
    "    'TCOLNAME': 'description',\n",
    "    'CCLASS': 'data_class',\n",
    "#    'TEXTRA': 'extra information',\n",
    "    'TBCREATOR': 'schema',\n",
    "    'FORMAT': 'format',\n",
    "    'KEYSEQ': 'key_sequence',\n",
    "    'COLNO': 'column_number'\n",
    "    }\n",
    "converter_rz1 = {'CTABNAME': 'table_name', 'ALTER_ID': 'alter_id','TTID': \"description\", 'CTID':'table_id'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ingest RZ1 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'rz1{}'.format(environ_suffix) # the cloudant database name to be ingested\n",
    "query_rz1 = \"\"\"\n",
    "select CTID, CTABNAME, TTID, CTABNAME as ALTER_ID\n",
    "from odmprd.odmt_ddict_tables   \n",
    "where \n",
    "1 = 1\n",
    "--AND CTID = 'E02'\n",
    "\"\"\"\n",
    "\n",
    "res =  cc.odm_2_cloudant(query_rz1, 'prod', cloudant_db_name, \n",
    "                         converter = converter_rz1, \n",
    "                         keys = ['CTID'] , \n",
    "                         mode = 'REPLACE', \n",
    "                         src_code = 'rz1',\n",
    "                         drop_origin_keys = True)\n",
    "print(res)\n",
    "\n",
    "# create dataframe for odmmeta\n",
    "with odm_conn.odm_adhoc('prod') as odmprd_adhoc: \n",
    "    result = odmprd_adhoc(query_rz1)\n",
    "    df_rz1 = pd.DataFrame(result).rename(columns = converter_rz1)\n",
    "df_rz1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest RZ3 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'rz3{}'.format(environ_suffix) # the cloudant database name to be ingested\n",
    "query_rz3 = \"\"\"\n",
    "select CTID, CTABNAME, TTID , CTABNAME as ALTER_ID\n",
    "from odmprd.ODMT_DDICT_TBLODMR   \n",
    "where \n",
    "1 = 1\n",
    "--AND CTID = 'E02'\n",
    "\"\"\"\n",
    "res =  cc.odm_2_cloudant(query_rz3, 'prod', cloudant_db_name, \n",
    "                         converter = converter_rz1, \n",
    "                         keys = ['CTID'] , \n",
    "                         mode = 'REPLACE', \n",
    "                         src_code = 'rz3')\n",
    "print(res)\n",
    "\n",
    "# create dataframe for odmmeta\n",
    "with odm_conn.odm_adhoc('prod') as odmprd_adhoc: \n",
    "    result = odmprd_adhoc(query_rz3)\n",
    "    df_rz3 = pd.DataFrame(result).rename(columns = converter_rz1)\n",
    "df_rz3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest RZ2 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctid_list = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "gp_cnt = 4\n",
    "gps = [ctid_list[start::gp_cnt] for start in range(gp_cnt)]\n",
    "gps_cond = [ ', '.join(  [\"'{}'\".format(ch)      for ch in gp ]) for gp in gps]\n",
    "gps_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'rz2{}'.format(environ_suffix)\n",
    "cc.cloudant_client.delete_database(cloudant_db_name) # remove the original contents todo full load-replacement\n",
    "for gp_cond in gps_cond: \n",
    "    query_rz2 = \"\"\"\n",
    "    select RZ2.CTID, \n",
    "    RZ2.CCOLNAME, \n",
    "    RZ2.TCOLNAME,\n",
    "    TRIM(RZ1.CTABNAME) || '.'||TRIM(RZ2.CCOLNAME) AS ALTER_ID, \n",
    "    RZ2.CCLASS,  \n",
    "    --RZ2.TEXTRA, \n",
    "    RZ1.CTABNAME, \n",
    "    SYC.TBCREATOR, \n",
    "    SYC.COLNO, \n",
    "    SYC.COLTYPE|| '('||SYC.LENGTH||')' AS FORMAT,\n",
    "    SYC.KEYSEQ\n",
    "    from \n",
    "    odmprd.odmt_ddict_columns RZ2, \n",
    "    ODMPRD.odmt_ddict_tables RZ1,\n",
    "    sysibm.syscolumns SYC\n",
    "    where \n",
    "    RZ2.fdiscont <> 'Y' \n",
    "    and RZ2.CLANGUAG = '' \n",
    "    AND RZ1.CTID = RZ2.CTID \n",
    "    and SYC.TBNAME = RZ1.CTABNAME\n",
    "    and SYC.NAME = RZ2.CCOLNAME\n",
    "    AND SYC.TBCREATOR = 'ODMPRD'\n",
    "    AND SUBSTR(RZ1.CTID,1,1) IN ({})\n",
    "    --AND RZ1.CTID = 'E02'\n",
    "    \"\"\".format(gp_cond)\n",
    "\n",
    "    res = cc.odm_2_cloudant(query_rz2, 'prod', cloudant_db_name,  \n",
    "                         converter = converter_rz2, \n",
    "                         keys = ['CTID', 'CCOLNAME'] , \n",
    "                         mode = 'APPEND',     # Must be append for RZ2 and RZ4\n",
    "                         src_code = 'rz2',\n",
    "                         drop_origin_keys = False)\n",
    "    print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rz2 = \"\"\"\n",
    "select RZ2.CTID, \n",
    "RZ2.CCOLNAME, \n",
    "RZ2.TCOLNAME,\n",
    "TRIM(RZ1.CTABNAME) || '.'||TRIM(RZ2.CCOLNAME) AS ALTER_ID, \n",
    "RZ2.CCLASS,  \n",
    "--RZ2.TEXTRA, \n",
    "RZ1.CTABNAME, \n",
    "SYC.TBCREATOR, \n",
    "SYC.COLNO, \n",
    "SYC.COLTYPE|| '('||SYC.LENGTH||')' AS FORMAT,\n",
    "SYC.KEYSEQ\n",
    "from \n",
    "odmprd.odmt_ddict_columns RZ2, \n",
    "ODMPRD.odmt_ddict_tables RZ1,\n",
    "sysibm.syscolumns SYC\n",
    "where \n",
    "RZ2.fdiscont <> 'Y' \n",
    "and RZ2.CLANGUAG = '' \n",
    "AND RZ1.CTID = RZ2.CTID \n",
    "and SYC.TBNAME = RZ1.CTABNAME\n",
    "and SYC.NAME = RZ2.CCOLNAME\n",
    "AND SYC.TBCREATOR = 'ODMPRD'\n",
    "\"\"\"\n",
    "\n",
    "# create dataframe for odmmeta\n",
    "with odm_conn.odm_adhoc('prod') as odmprd_adhoc: \n",
    "    result = odmprd_adhoc(query_rz2)\n",
    "    df_rz2 = pd.DataFrame(result).rename(columns = converter_rz2)\n",
    "df_rz2.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rz2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest RZ4 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctid_list = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "gp_cnt = 8\n",
    "gps = [ctid_list[start::gp_cnt] for start in range(gp_cnt)]\n",
    "gps_cond = [ ', '.join(  [\"'{}'\".format(ch)      for ch in gp ]) for gp in gps]\n",
    "gps_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'rz4{}'.format(environ_suffix)\n",
    "cc.cloudant_client.delete_database(cloudant_db_name) # full load replacement\n",
    "for gp_cond in gps_cond: \n",
    "    query_rz4 = \"\"\"\n",
    "    select RZ4.CTID, \n",
    "    RZ4.CCOLNAME, \n",
    "    RZ4.TCOLNAME, \n",
    "    TRIM(RZ3.CTABNAME) || '.'||TRIM(RZ4.CCOLNAME) AS ALTER_ID, \n",
    "    RZ4.CCLASS,  \n",
    "    --RZ4.TEXTRA, \n",
    "    RZ3.CTABNAME, \n",
    "    SYC.TBCREATOR, \n",
    "    SYC.COLNO, \n",
    "    SYC.COLTYPE|| '('||SYC.LENGTH||')' AS FORMAT,\n",
    "    SYC.KEYSEQ\n",
    "    from \n",
    "    odmprd.ODMT_DDICT_COLODMR RZ4, \n",
    "    ODMPRD.ODMT_DDICT_TBLODMR RZ3,\n",
    "    sysibm.syscolumns SYC\n",
    "    where \n",
    "    RZ4.fdiscont <> 'Y' \n",
    "    and RZ4.CLANGUAG = '' \n",
    "    AND RZ3.CTID = RZ4.CTID \n",
    "    and SYC.TBNAME = RZ3.CTABNAME\n",
    "    and SYC.NAME = RZ4.CCOLNAME\n",
    "    AND SYC.TBCREATOR = 'IDMPRD'\n",
    "    AND SUBSTR(RZ3.CTID, 1,1) IN ({})\n",
    "    --AND RZ3.CTID = 'E02'\n",
    "    \"\"\".format(gp_cond)\n",
    "    res = cc.odm_2_cloudant(query_rz4, 'prod', cloudant_db_name,  \n",
    "                         converter = converter_rz2, \n",
    "                         keys = ['CTID', 'CCOLNAME'] , \n",
    "                         mode = 'APPEND', \n",
    "                         src_code = 'rz4',\n",
    "                         drop_origin_keys = False)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for odmmeta database\n",
    "query_rz4 = \"\"\"\n",
    "select RZ4.CTID, \n",
    "RZ4.CCOLNAME, \n",
    "RZ4.TCOLNAME, \n",
    "TRIM(RZ3.CTABNAME) || '.'||TRIM(RZ4.CCOLNAME) AS ALTER_ID, \n",
    "RZ4.CCLASS,  \n",
    "--RZ4.TEXTRA, \n",
    "RZ3.CTABNAME, \n",
    "SYC.TBCREATOR, \n",
    "SYC.COLNO, \n",
    "SYC.COLTYPE|| '('||SYC.LENGTH||')' AS FORMAT,\n",
    "SYC.KEYSEQ\n",
    "from \n",
    "odmprd.ODMT_DDICT_COLODMR RZ4, \n",
    "ODMPRD.ODMT_DDICT_TBLODMR RZ3,\n",
    "sysibm.syscolumns SYC\n",
    "where \n",
    "RZ4.fdiscont <> 'Y' \n",
    "and RZ4.CLANGUAG = '' \n",
    "AND RZ3.CTID = RZ4.CTID \n",
    "and SYC.TBNAME = RZ3.CTABNAME\n",
    "and SYC.NAME = RZ4.CCOLNAME\n",
    "AND SYC.TBCREATOR = 'IDMPRD'\n",
    "\"\"\"\n",
    "with odm_conn.odm_adhoc('prod') as odmprd_adhoc: \n",
    "    result = odmprd_adhoc(query_rz4)\n",
    "    df_rz4 = pd.DataFrame(result).rename(columns = converter_rz2)\n",
    "df_rz4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Ingest Business Definition Table   \n",
    "\n",
    "\n",
    "#### Now ingest the business definition document  \n",
    "\n",
    "the business definition document is in the following link:   \n",
    "https://apps.na.collabserv.com/communities/service/html/communityview?communityUuid=84d3cd10-4160-48e1-abb6-f65af679ee28#fullpageWidgetId=Wabd63ea0f599_4dc3_aed5_9ffce68576e0&file=ca75dc47-c7f3-4fde-8249-3e2bc7b05a9b\n",
    "\n",
    "**move the business definition document into the following folder:**  \n",
    "~/Documents/GitHub/jupyter/input/Business_Definitions.docx\n",
    "\n",
    "``` \n",
    "rm ~/Documents/GitHub/jupyter/input/Business_Definitions.docx\n",
    "mv ~/Downloads/\"ODM Business Definitions.docx\"  ~/Documents/GitHub/jupyter/input/Business_Definitions.docx \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'busi_def{}'.format(environ_suffix)\n",
    "busi_file = '/input/Business_Definitions.docx'\n",
    "text = docx2txt.process(busi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_strip to remove the leading blank and tailing blank , blank lines. \n",
    "def f_strip(x):\n",
    "    y = {}\n",
    "    for item in x: \n",
    "        y[item] = x[item].strip().replace('\\n\\n', '\\n')\n",
    "    return y\n",
    "# remove the head and tail,  4 # is the mark added manually in the document and it is hidden in the file with white color \n",
    "p = re.compile(r'.*#{4}(.*)End\\sof\\sDocument', re.DOTALL )\n",
    "newText= p.match(text).groups()[0]\n",
    "p = re.compile(r'\\n\\s+') \n",
    "newText = p.sub('\\n', newText)\n",
    "p = re.compile(r'\\n+') \n",
    "newText = p.sub('\\n', newText)\n",
    "# replace valid values: to be Allowed Values:\n",
    "p = re.compile(r'valid values\\s*:', re.I)\n",
    "newText = p.sub('Allowed Values:', newText)\n",
    "# use findall to split the text by business terms\n",
    "x=re.compile(r'.*?Business\\s+definition\\:.*?Format\\:.*?Allowed\\s+Values\\:.*?(?=\\n[^\\n]*\\nBusiness\\s+definition\\:)', re.S|re.I)\n",
    "lst = x.findall(newText+ '\\n \\nBusiness definition:') # attach str 'business definition' so that the last business term can be matched\n",
    "# remove the blank lines \n",
    "p = re.compile('^$\\n', re.DOTALL|re.MULTILINE)\n",
    "newLst = list(map(lambda y: p.sub('', y), lst))\n",
    "#parse every business term block to get term name, definition, format, and allowed values\n",
    "p = re.compile(r'(.*)Business\\s+definition\\:(.*?)Format\\:(.*?)Allowed\\s+Values\\:(.*)', re.S|re.I) \n",
    "newLst = list(map(lambda x: p.match(x).groups(), newLst))\n",
    "# get the short description and term name from the first element in each item\n",
    "# the parsed item will be 'short desc', 'term name', 'tail'. the short desc + tail will become final short desc\n",
    "p = re.compile(r'(.*)\\(\\s*(.*)\\s*\\)(.*)')\n",
    "def f(x): # f(x) is to parse the title line of every business term to get the short description and the term name\n",
    "    y=p.match(x[0])\n",
    "    if y: \n",
    "        z = y.groups() + x[1:]\n",
    "    else: \n",
    "        print('\\n' )\n",
    "        print(str( x))\n",
    "        z = ('NA', 'NA')+ x[1:]\n",
    "    return z\n",
    "newLst = list(map(f, newLst))\n",
    "\n",
    "fields = [ 'Term Name','Short Description', 'Long Description', 'Data Type', 'Allowed Values']\n",
    "busiTermDictList = list(map(lambda x: dict(zip(fields,(x[1], x[0]+x[2]) + x[3:])), newLst))\n",
    "busiTermDictList = list(map(f_strip, busiTermDictList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busi_df = pd.DataFrame(busiTermDictList)\n",
    "converter_busi = {'Long Description': 'long_description', \n",
    "                  'Short Description': 'description', \n",
    "                  'Data Type': 'format', \n",
    "                 'Allowed Values': 'allowed_values'}\n",
    "busi_df = busi_df.loc[busi_df[\"Term Name\"] !='']\n",
    "# get the duplicate term in the list\n",
    "busi_df.set_index('Term Name').loc[busi_df.groupby('Term Name')['Short Description'].count()==2].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busiTermDictList_new = [{'_'.join(map(lambda s: s.upper(), k.split())):v   for k, v in element.items()}for element in busiTermDictList]\n",
    "busiTermDictList_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for element in busiTermDictList_new: \n",
    "    print(element['TERM_NAME'].replace('/', '_'))\n",
    "    with open('/result/busi_df/{}.json'.format(element['TERM_NAME']).replace('/', '_'), 'w') as f: \n",
    "        json.dump(element, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busi_df.to_excel('busi_def.xlsx', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.df_2_cloudant(busi_df, cloudant_db_name, \n",
    "                 keys = ['Term Name'], \n",
    "                 converter = converter_busi, \n",
    "                 mode = 'REPLACE', \n",
    "                src_code = 'busi_def',\n",
    "                drop_origin_keys = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbname is the df using tbname as key to be ingested in cloudant db\n",
    "df_rz13_tbname = pd.concat([df_rz1, df_rz3]).applymap(lambda x: x.strip()).drop(columns = ['alter_id']) \n",
    "def process_df13(df):\n",
    "    return df.apply(lambda col: ', '.join(col.drop_duplicates()))\n",
    "df_rz13_tid = df_rz13_tbname.groupby('table_id').apply(process_df13).drop(columns = ['table_id']).reset_index()\n",
    "#df_rz13_tid.loc[df_rz13_tid.table_id == 'D01']\n",
    "#df_rz13_tbname.loc[df_rz13_tbname.table_id == 'E02']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['table_id', 'column_name', 'description', 'data_class','table_name','format', 'schema']\n",
    "lst = list(busi_df)\n",
    "busi_df[lst] = busi_df[lst].astype(str)\n",
    "busi_df = busi_df.applymap(lambda x: x.strip())\n",
    "cols = [ 'column_name', 'description', 'data_class','table_name','format']\n",
    "\n",
    "df_rz24 = pd.concat([df_rz2, df_rz4])\n",
    "df_rz24[list(df_rz24)] = df_rz24[list(df_rz24)].astype(str).applymap(lambda x: x.strip())\n",
    "df_rz24.table_name = + df_rz24.table_id + ' ' + df_rz24.schema + '.' + df_rz24.table_name\n",
    "df_rz24 = df_rz24.reindex(columns = cols)\n",
    "#df_rz24_test = df_rz24.loc[df_rz24.column_name == 'CFORMSTA']\n",
    "df_rz24 = df_rz24.merge(busi_df, left_on = 'column_name', right_on = 'Term Name', how = 'left').drop(columns = ['Term Name', 'Short Description', 'Data Type']).fillna('')\n",
    "\n",
    "\n",
    "def process_df_rz24(df):\n",
    "    ser = pd.Series()\n",
    "    for col in df.columns:\n",
    "        if col == 'table_name':\n",
    "            ser[col] = ', '.join(df[col].drop_duplicates())\n",
    "        else:\n",
    "            ser[col] = df[col].value_counts().idxmax()\n",
    "    return ser\n",
    "df_rz24_new = df_rz24.groupby('column_name').apply(process_df_rz24)\n",
    "df_rz24_new = df_rz24_new.drop(columns = ['column_name']).reset_index()\n",
    "df_rz24_new.rename(columns = {'table_name': 'where'}, inplace = True)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('\\n'.join(df_rz24_new.loc[(df_rz24_new.column_name == 'CSALBIW'), ['table_name']].table_name.values[0].split(',') ))\n",
    "# df_rz24.loc[(df_rz24.column_name == 'CSALBIW')]['table_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rz24_new.loc[df_rz24_new.column_name == 'RCNUM'].T\n",
    "\n",
    "#df_rz24_new = df_rz24_new.merge(busi_df, left_on = 'column_name', right_on = 'Term Name', how = 'left').drop(columns = ['Term Name', 'Short Description', 'Data Type'])\n",
    "# there are duplicated in busi_df, just run the follow code to remove the duplicates\n",
    "#df_rz24_new = df_rz24_new.groupby('column_name').apply(process_df_rz24)\n",
    "#df_rz24_new = df_rz24_new.drop(columns = ['column_name']).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Glossary Table  \n",
    "\n",
    "\n",
    "#### now load the glos document\n",
    "\n",
    "the glossary document is in the following link: \n",
    "https://apps.na.collabserv.com/communities/service/html/communityview?communityUuid=84d3cd10-4160-48e1-abb6-f65af679ee28#fullpageWidgetId=Wabd63ea0f599_4dc3_aed5_9ffce68576e0&file=c9acb4a9-a5e8-43d4-8687-1e2bfcf4ce9d\n",
    "\n",
    "use the following commands to move it the working folder:\n",
    "```\n",
    "rm ~/Documents/GitHub/jupyter/input/Glossary_1.xlsx\n",
    "mv ~/Downloads/\"Glossary of Acronyms 2019-06-27.xlsx\"  ~/Documents/GitHub/jupyter/input/Glossary_1.xlsx \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_db_name = 'glossary{}'.format(environ_suffix)\n",
    "glos_file = '/input/Glossary_1.xlsx'\n",
    "df_glos = pd.read_excel(glos_file).fillna('')\n",
    "df_glos = df_glos.rename(index=str, columns = {\"Acronym/Term\": \"Term Name\"}) \n",
    "df_glos = df_glos.loc[~df_glos['Term Name'].isin(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')) ]\n",
    "df_glos['description'] = df_glos.Explanation + '\\n' + df_glos.Description\n",
    "df_glos.drop(columns = {'Explanation', 'Description'}, inplace = True)\n",
    "cc.df_2_cloudant(df_glos, cloudant_db_name, \n",
    "                 keys = ['Term Name'], \n",
    "                 mode = 'REPLACE', \n",
    "                src_code = 'glossary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's populate the odmmeta database.\n",
    "we have \n",
    "+ df_rz13_tbname\n",
    "+ df_rz13.tid\n",
    "+ df_rz24_new\n",
    "+ df_glos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rz13_tbname['category'] = 'table'\n",
    "df_rz13_tid['category'] = 'table'\n",
    "df_glos['category'] = 'glossory'\n",
    "df_rz24_new['category'] = 'column'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rz13_tbname.rename(columns = {'table_name': '_id'}, inplace = True)\n",
    "df_rz13_tid.rename(columns = {'table_id': '_id'}, inplace = True)\n",
    "df_glos.rename(columns = {'Term Name': '_id'}, inplace = True)\n",
    "rz13_tbname_dict = df_rz13_tbname.to_dict(orient = 'record')\n",
    "rz13_tid_dict = df_rz13_tid.to_dict(orient = 'record')\n",
    "glos_dict = df_glos.to_dict(orient = 'record')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rz24_new = df_rz24_new.rename(columns = {'column_name': '_id'})\n",
    "rz24_rows = df_rz24_new.fillna('').to_dict(orient = 'record')\n",
    "rz24_rows_dict = [ dict(filter(lambda x: x[1] != '' , row.items())) for row in rz24_rows]\n",
    "rz24_rows_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rz24_new.loc[df_rz24_new._id == 'CFORMSTA'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_db_name = 'odmmeta'\n",
    "cc.cloudant_client.connect()\n",
    "cc.cloudant_client.delete_database(cloud_db_name)\n",
    "my_database = cc.cloudant_client.create_database(cloud_db_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_database.bulk_docs(rz13_tbname_dict)\n",
    "my_database.bulk_docs(rz13_tid_dict)\n",
    "my_database.bulk_docs(rz24_rows_dict)\n",
    "my_database.bulk_docs(glos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INGESTION STOPS HERE\n",
    "\n",
    "\n",
    "## Access cloudant database via python APIs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#cloudant_db_name = 'busi_test'\n",
    "cc.cloudant_client.connect()\n",
    "#cloudant_db_names = {'rz1':'rz1_test', \n",
    "#                     'rz2':'rz2_test', \n",
    "#                     'rz3':'rz3_test', \n",
    "#                     'rz4':'rz4_test', \n",
    "#                     'busi':'busi_def_test', \n",
    "#                     'glos':'glossary_test'}\n",
    "cloudant_db_names = {'rz3':'rz3{}'.format(environ_suffix), \n",
    "                     'rz4':'rz4{}'.format(environ_suffix), \n",
    "                     'busi':'busi_def{}'.format(environ_suffix), \n",
    "                     'glos':'glossary{}'.format(environ_suffix)}\n",
    "mydfs= {key: pd.DataFrame(list(cc.cloudant_client[db_name])).drop(columns = ['_rev']) for key, db_name in cloudant_db_names.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mydf_all = pd.concat(mydfs, ignore_index = True, sort = True).fillna('')\n",
    "mydf_all.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "mydfs['rz3']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "mydf_all.loc[mydf_all.src.isin(['rz2', 'rz4'])].sort_values(['_id', 'src'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df_temp = mydfs['rz2'].merge(mydfs['busi'].loc[:, ['_id', 'description', 'allowed_values']], \n",
    "                             left_on = 'column_name', right_on = '_id', how = 'left', suffixes=['', '_from_busi_def'] ).fillna('')\n",
    "df_temp.loc[df_temp._id == 'ODMT_EMPLOYEE.RCNUM'].T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = mydfs['rz2']\n",
    "df.loc[df.description.str.lower().str.contains('org') & df['table_id'].str.startswith('E')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfy_rz2 = dfy.loc[dfy.src == 'RZ2']\n",
    "dfy_rz2.loc[(dfy_rz2['table_id'] == 'E12') & dfy_rz2.description.str.lower().str.contains('school')]\n",
    "#dfy_rz2.loc[dfy_rz2.description.str.lower().str.contains('education')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dfy_rz1 = dfy.loc[dfy.src == 'RZ1']\n",
    "dfy_rz1.loc[(dfy_rz1['table id'] == 'E12') ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfy_bus = dfy.loc[dfy.src =='busi_def']\n",
    "dfy_bus.loc[dfy_bus._id.str.contains('COST')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mydb['ODMT_EMPLOYEE.RCNUM']\n",
    "xx = filter(lambda item: re.search(r'business\\s+unit', item['description'], re.I), mydb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for x in xx:\n",
    "    print(x['_id'], x['description'], x.get('table id', '') ,'\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfx= pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfx.fillna('', inplace = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfx.drop(columns = ['_rev'], inplace = True)\n",
    "dfx.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfx.loc[dfx.description.str.contains('salary'), ['_id', 'table id', 'description']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spi_docs = filter(lambda doc: doc.get('WSF', '') == 'SPI', docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chk_desc = list(filter(lambda doc: re.search(r'BU', doc['description']), docs))\n",
    "print(chk_desc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#for item in chk_desc:\n",
    "#    print(\"{:6s}: {:40s} {:100s}\".format( item['table id'], item['_id'], item['description']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cloudant_db_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfx.loc[(dfx.description.str.upper().str.contains('SCHOOL'))]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(dfx.loc[(dfx['src'] == 'RZ2') & (dfx['table id'] == 'E12')]['_id'].values)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rz4_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
